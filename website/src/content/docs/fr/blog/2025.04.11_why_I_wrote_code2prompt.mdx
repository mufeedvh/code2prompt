---
title: "Pourquoi j'ai d√©velopp√© Code2Prompt"
date: 2025-04-11
lastUpdated: 2025-04-11
tags:
  - open-source
  - code2prompt
  - AI
  - Agent
excerpt: "L'histoire derri√®re code2prompt : ma qu√™te open-source pour relever les d√©fis de contexte dans les flux de travail LLM"
authors:
  - ODAncona
cover:
  alt: "Une illustration de code2prompt rationalisant le contexte de code pour les agents AI."
  image: "../../../../assets/logo_dark_v0.0.2.svg"
featured: false
draft: false
---

## Introduction

J'ai toujours √©t√© fascin√© par la fa√ßon dont les mod√®les de langage √† grande √©chelle (LLM) transforment les flux de travail de codage - en g√©n√©rant des tests, des docstrings ou m√™me en exp√©diant des fonctionnalit√©s enti√®res en quelques minutes. Mais √† mesure que je poussais ces mod√®les plus loin, quelques points de douleur critiques ont continu√© √† √©merger :

| Difficult√©s de planification | Co√ªts √©lev√©s de tokens | Hallucinations |
| ---------------------------- | ---------------------- | -------------- |
| üß† ‚û°Ô∏è ü§Ø                     | üî• ‚û°Ô∏è üí∏               | üí¨ ‚û°Ô∏è üåÄ       |

C'est pourquoi j'ai commenc√© √† contribuer √† `code2prompt`, un outil bas√© sur Rust pour aider √† fournir le contexte appropri√© aux LLM.

Dans cet article, je partagerai mon parcours et expliquerai pourquoi je suis convaincu que `code2prompt` est pertinent aujourd'hui et s'int√®gre si bien, et pourquoi il est devenu ma solution incontournable pour des flux de travail de codage AI plus rapides et plus efficaces.

## Mes premiers pas avec les LLM üë£

J'ai commenc√© √† exp√©rimenter avec les LLM sur `OpenAI Playground` avec `text-davinci-003` lorsqu'il a gagn√© en traction en novembre 2023. Les mod√®les de langage ont permis une nouvelle r√©volution. Cela ressemblait √† avoir un assistant brillant qui pouvait produire des tests unitaires et des docstrings presque sur commande. J'ai appr√©ci√© pousser les mod√®les √† leurs limites - en testant tout, des conversations l√©g√®res et des dilemmes √©thiques aux jailbreaks et aux t√¢ches de codage complexes. Cependant, √† mesure que j'ai pris des projets plus importants, j'ai rapidement r√©alis√© que les mod√®les avaient des limitations √©videntes. Au d√©but, je ne pouvais int√©grer que quelques centaines de lignes de code dans la fen√™tre de contexte, et m√™me alors, les mod√®les avaient souvent du mal √† comprendre le but ou la structure du code. C'est pourquoi j'ai rapidement remarqu√© que l'importance du contexte √©tait primordiale. Plus mes instructions √©taient concises et meilleur √©tait le contexte, meilleurs √©taient les r√©sultats.

![OpenAI Playground](/assets/blog/post1/playground.png)

## √âvolution des mod√®les üèóÔ∏è

Les mod√®les pouvaient produire des r√©sultats impressionnants mais avaient souvent du mal avec des bases de code plus importantes ou des t√¢ches complexes. Je me suis retrouv√© √† passer plus de temps √† cr√©er des invites qu'√† coder. Dans le m√™me temps, les mod√®les ont continu√© √† s'am√©liorer avec la sortie de nouvelles versions. Ils ont augment√© leurs capacit√©s de raisonnement et la taille du contexte, offrant de nouvelles perspectives et possibilit√©s. Je pouvais int√©grer presque deux mille lignes de code dans la fen√™tre de contexte, et les r√©sultats se sont am√©lior√©s. Je pouvais √©crire des fonctionnalit√©s enti√®res en quelques it√©rations, et j'ai √©t√© √©tonn√© de la rapidit√© avec laquelle je pouvais obtenir des r√©sultats. J'√©tais convaincu que les LLM √©taient l'avenir du codage, et je voulais en faire partie. Je crois fermement que l'IA ne remplacera pas les humains, mais les assistera sous la forme d'assistants o√π les humains sont toujours aux commandes.

## Mes premiers projets avec les LLM üöÄ

J'ai commenc√© √† √©crire un module de recherche de chemin pour une comp√©tition robotique `ROS`, √† g√©n√©rer des fonctionnalit√©s pour une application `Flutter` multiplateforme d'architecture propre, et j'ai cr√©√© une petite application web en une soir√©e pour suivre mes d√©penses en `Next.js`, un framework que je n'avais jamais utilis√© auparavant ! Le fait que j'ai construit cette petite application en une soir√©e, dans un framework que je n'avais jamais touch√© auparavant, a √©t√© un moment r√©v√©lateur pour moi ; les LLM n'√©taient pas seulement des outils mais des multiplicateurs. J'ai d√©velopp√© `bboxconverter`, un package pour convertir des bo√Ætes de bounding, et la liste continue. Les LLM peuvent aider √† apprendre de nouvelles technologies et frameworks rapidement ; c'est formidable.

## Un nouveau paradigme : Software 3.0 üí°

Je me suis approfondi dans les LLM et j'ai commenc√© √† construire des agents et des scaffoldings autour d'eux. J'ai reproduit le c√©l√®bre article [RestGPT](https://restgpt.github.io/). L'id√©e est excellente : donner aux LLM la capacit√© d'appeler certaines API REST avec une sp√©cification OpenAPI, comme `Spotify` ou `TMDB`. Ces capacit√©s introduisent un nouveau paradigme de programmation logiciel, que j'aime appeler **Software 3.0**.

| Software 1.0        | Software 2.0           | Software 3.0 |
| ------------------- | ---------------------- | ------------ |
| Bas√© sur des r√®gles | Pilot√© par les donn√©es | Agence       |

La m√™me id√©e a propuls√© le protocole [MCP](https://modelcontextprotocol.io/introduction), qui permet aux LLM d'appeler des outils et des ressources directement de mani√®re transparente, car par conception, l'outil a besoin d'une description pour √™tre appel√© par le LLM, contrairement aux API REST qui ne n√©cessitent pas n√©cessairement de sp√©cification OpenAPI.

## Les limites des LLM üß©

### Hallucinations üåÄ

En reproduisant le c√©l√®bre article `RESTGPT`, j'ai remarqu√© certaines limitations s√©rieuses des LLM. Les auteurs de l'article ont rencontr√© les m√™mes probl√®mes que moi : les LLM **hallucinaient**. Ils g√©n√®rent du code qui n'est pas impl√©ment√©, inventant des arguments et suivant simplement les instructions √† la lettre sans utiliser le bon sens. Par exemple, dans le code source original de RestGPT, les auteurs ont demand√© dans [l'invite de l'appelant](https://github.com/Yifan-Song793/RestGPT/blob/main/model/caller.py).

> "ne pas √™tre trop intelligent et inventer des √©tapes qui n'existent pas dans le plan."

J'ai trouv√© cette d√©claration amusante et tr√®s int√©ressante car c'√©tait la premi√®re fois que je rencontrais quelqu'un qui instruisait les LLM √† ne pas halluciner.

### Taille limit√©e du contexte üìè

Une autre limitation √©tait la taille du contexte ; les LLM fonctionnent bien pour trouver l'aiguille dans la botte de foin mais ont du mal √† en comprendre le sens. Lorsque vous donnez trop de contexte aux mod√®les de langage, ils ont tendance √† se perdre dans les d√©tails et √† perdre de vue l'image d'ensemble, ce qui est frustrant et n√©cessite une direction constante. La fa√ßon dont j'aime y penser est similaire √† [la mal√©diction de la dimensionnalit√©](https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb/). Remplacez le mot "dimension" ou "caract√©ristique" par "contexte", et vous obtenez l'id√©e.

![Mal√©diction de la dimensionnalit√©](/assets/blog/post1/curse_of_dimensionality.png)

Plus vous donnez de contexte au LLM, plus il est difficile de trouver la bonne r√©ponse. J'ai cr√©√© une phrase pour r√©sumer cette id√©e :

> Fournissez aussi peu de contexte que possible mais autant que n√©cessaire

Ceci est fortement inspir√© de la c√©l√®bre [citation d'Alain Berset](https://www.lematin.ch/story/alain-berset-la-formule-qui-defie-le-temps-166189802108), un politicien suisse üá®üá≠ qui a d√©clar√© pendant le confinement COVID-19 :

> "Nous souhaitons agir aussi vite que possible, mais aussi lentement que n√©cessaire"

Cela repr√©sente l'id√©e de compromis et s'applique √† la taille du contexte des LLM !

## Recherche d'une meilleure fa√ßon : code2prompt üî®

Par cons√©quent, j'avais besoin d'un moyen de charger, de filtrer et d'organiser rapidement mon contexte de code en fournissant la moindre quantit√© possible de contexte avec la meilleure qualit√© possible. J'ai essay√© de copier manuellement des fichiers ou des extraits dans des invites, mais cela est devenu encombrant et sujet aux erreurs. Je savais que l'automatisation du processus fastidieux de forgeage du contexte pour poser de meilleures invites serait utile. Ensuite, un jour, j'ai tap√© "code2prompt" sur Google, en esp√©rant trouver un outil qui pipe mon code directement dans des invites.

Et voil√†, j'ai d√©couvert un projet **bas√© sur Rust** par [Mufeed](https://www.reddit.com/r/rust/comments/1bghroh/i_made_code2prompt_a_cli_tool_to_convert_your/) nomm√© _code2prompt_, avec environ 200 √©toiles sur GitHub. C'√©tait encore basique √† l'√©poque : un simple outil CLI avec une capacit√© de filtrage limit√©e et des mod√®les. J'ai vu un √©norme potentiel et j'ai saut√© dedans pour contribuer, en impl√©mentant la correspondance de mod√®les globaux, entre autres fonctionnalit√©s, et je suis rapidement devenu l'un des principaux contributeurs.

## Vision & Int√©grations üîÆ

Aujourd'hui, il existe plusieurs fa√ßons de fournir un contexte aux LLM. G√©n√©rer √† partir du contexte plus large, utiliser la g√©n√©ration augment√©e de r√©cup√©ration (RAG), [compresser le code](https://www.all-hands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents), ou m√™me utiliser une combinaison de ces m√©thodes. Le forgeage de contexte est un sujet br√ªlant qui √©voluera rapidement dans les mois √† venir. Cependant, mon approche est **KISS** : Keep It Simple, Stupid. La meilleure fa√ßon de fournir un contexte aux LLM est d'utiliser la fa√ßon la plus simple et la plus efficace possible. Vous forgez pr√©cis√©ment le contexte dont vous avez besoin ; c'est d√©terministe, contrairement √† RAG.

C'est pourquoi j'ai d√©cid√© de pousser `code2prompt` plus loin en tant qu'outil simple pouvant √™tre utilis√© dans n'importe quel flux de travail. Je voulais le rendre facile √† utiliser, facile √† int√©grer et facile √† √©tendre. C'est pourquoi j'ai ajout√© de nouvelles fa√ßons d'interagir avec l'outil.

- **Core** : Le c≈ìur de `code2prompt` est une biblioth√®que Rust qui fournit la fonctionnalit√© de base pour forger le contexte √† partir de votre base de code. Il comprend une API simple pour charger, filtrer et organiser votre contexte de code.
- **CLI :** L'interface de ligne de commande est la fa√ßon la plus simple d'utiliser `code2prompt`. Vous pouvez forger le contexte √† partir de votre base de code et le pipe directement dans vos invites.
- **API Python :** L'API Python est un simple wrapper autour de CLI qui vous permet d'utiliser `code2prompt` dans vos scripts Python et agents. Vous pouvez forger le contexte √† partir de votre base de code et le pipe directement dans vos invites.
- **MCP :** Le serveur MCP de `code2prompt` permet aux LLM d'utiliser `code2prompt` comme outil, les rendant ainsi capables de forger le contexte.

La vision est d√©crite plus en d√©tail dans la [page de vision](/docs/vision) de la documentation.

## Int√©gration avec les agents üë§

Je crois que les futurs agents auront besoin d'un moyen d'ing√©rer le contexte, et `code2prompt` est la fa√ßon simple et efficace de le faire pour les r√©f√©rentiels textuels comme la base de code, la documentation ou les notes. Un endroit typique pour utiliser `code2prompt` serait dans une base de code avec des conventions de nommage significatives. Par exemple, dans l'architecture propre, il y a une claire s√©paration des pr√©occupations et des couches. Le contexte pertinent r√©side g√©n√©ralement dans diff√©rents fichiers et dossiers mais partagent le m√™me nom. C'est un cas d'utilisation parfait pour `code2prompt`, o√π vous pouvez utiliser le mod√®le global pour saisir les fichiers pertinents.

**Glob Pattern-first :** S√©lectionnez ou excluez pr√©cis√©ment les fichiers avec un minimum de tracas.

En outre, la biblioth√®que principale est con√ßue comme un gestionnaire de contexte √©tatique, vous permettant d'ajouter ou de supprimer des fichiers √† mesure que votre conversation avec le LLM √©volue. Ceci est particuli√®rement utile pour fournir un contexte pour une t√¢che ou un objectif sp√©cifique. Vous pouvez facilement ajouter ou supprimer des fichiers du contexte sans relancer le processus.

**Contexte √©tatique :** Ajoutez ou supprimez des fichiers √† mesure que votre conversation avec le LLM √©volue.

Ces capacit√©s font de `code2prompt` un ajustement parfait pour les flux de travail bas√©s sur des agents. Le serveur MCP permet une int√©gration transparente avec des frameworks d'agents AI populaires comme [Aider](https://github.com/paul-gauthier/aider), [Goose](https://block.github.io/goose/), ou [Cline](https://github.com/jhillyerd/cline). Laissez-les g√©rer des objectifs complexes tandis que `code2prompt` fournit le contexte de code parfait.

## Pourquoi Code2prompt compte ‚úä

√Ä mesure que les LLM √©voluent et que les fen√™tres de contexte s'√©tendent, il peut sembler que simplement brutaliser des r√©f√©rentiels entiers dans des invites suffit. Cependant, **les co√ªts de tokens** et **la coh√©rence des invites** restent des obstacles importants pour les petites entreprises et les d√©veloppeurs. En se concentrant sur le code qui compte, `code2prompt` maintient votre utilisation de LLM efficace, rentable et moins encline √† l'hallucination.

**En r√©sum√© :**

- **R√©duisez les hallucinations** en fournissant la bonne quantit√© de contexte
- **R√©duisez les co√ªts de tokens** en curant manuellement le contexte appropri√© n√©cessaire
- **Am√©liorez les performances de LLM** en donnant la bonne quantit√© de contexte
- Int√®gre la pile agence en tant que fournisseur de contexte pour les r√©f√©rentiels textuels

## Vous pouvez rejoindre ! üåê

Chaque nouveau contributeur est le bienvenu ! Venez √† bord si vous √™tes int√©ress√© par Rust, la cr√©ation d'outils AI innovants, ou simplement si vous voulez un meilleur flux de travail pour vos invites bas√©es sur le code.

Merci de lire, et j'esp√®re que mon histoire vous a inspir√© √† d√©couvrir code2prompt. C'est √©t√© un incroyable parcours, et cela ne fait que commencer !

**Olivier D'Ancona**
