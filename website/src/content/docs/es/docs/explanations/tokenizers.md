---
title: Tokenizaci贸n en Code2Prompt
description: Aprende sobre la tokenizaci贸n y c贸mo Code2Prompt procesa texto para LLMs.
---

Cuando se trabaja con modelos de lenguaje, el texto debe transformarse en un formato que el modelo pueda entender: **tokens**, que son secuencias de n煤meros. Esta transformaci贸n se realiza mediante un **tokenizador**.

---

## 驴Qu茅 es un Tokenizador?

Un tokenizador convierte texto sin procesar en tokens, que son los bloques de construcci贸n para c贸mo los modelos de lenguaje procesan la entrada. Estos tokens pueden representar palabras, subpalabras o incluso caracteres individuales, dependiendo del dise帽o del tokenizador.

Para `code2prompt`, utilizamos el tokenizador **tiktoken**. Es eficiente, robusto y optimizado para modelos de OpenAI.
Puedes explorar su funcionalidad en el repositorio oficial

 [Repositorio de GitHub de tiktoken](https://github.com/openai/tiktoken)

Si deseas aprender m谩s sobre tokenizadores en general, consulta

 [Gu铆a de Tokenizaci贸n de Mistral](https://docs.mistral.ai/guides/tokenization/).

## Implementaci贸n en `code2prompt`

La tokenizaci贸n se implementa utilizando [`tiktoken-rs`](https://github.com/zurawiki/tiktoken-rs). `tiktoken` admite estos codificaciones utilizadas por los modelos de OpenAI:

| Argumento de CLI | Nombre de codificaci贸n  | Modelos de OpenAI                                                         |
|----|-----------------------| ------------------------------------------------------------------------- |
|`cl100k`| `cl100k_base`           | Modelos de ChatGPT, `text-embedding-ada-002`                              |
|`p50k`| `p50k_base`             | Modelos de c贸digo, `text-davinci-002`, `text-davinci-003`                 |
|`p50k_edit`| `p50k_edit`             | Utilizar para modelos de edici贸n como `text-davinci-edit-001`, `code-davinci-edit-001` |
|`r50k`| `r50k_base` (o `gpt2`) | Modelos de GPT-3 como `davinci`                                            |
|`gpt2`| `o200k_base`            | Modelos de GPT-4o                                                         |

Para obtener m谩s contexto sobre los diferentes tokenizadores, consulta [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/66b988407d8d13cad5060a881dc8c892141f2d5c/examples/How_to_count_tokens_with_tiktoken.ipynb)

> Esta p谩gina ha sido traducida autom谩ticamente para su conveniencia. Consulte la versi贸n en ingl茅s para ver el contenido original.
